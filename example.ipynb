{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import random\n",
    "from ew import EWBase, EWDense, EWConv2D\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_ew(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, EWBase):\n",
    "            layer.enable()\n",
    "\n",
    "def disable_ew(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, EWBase):\n",
    "            layer.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(x, y):\n",
    "    return tf.cast(x, tf.float32) / 255.0, y\n",
    "\n",
    "dataset = tfds.load(\"mnist\", as_supervised=True, split=\"train\")\n",
    "val_set = tfds.load(\"mnist\", as_supervised=True, split=\"test\")\n",
    "\n",
    "dataset = dataset.map(to_float)\n",
    "val_set = val_set.map(to_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the key set. In the paper they took a subset of the dataset and assigned random labels to them in order to combat query modification. However that altered the validation accuracy too much. For simplicity reasons we will just invert the pixels of a subset of the training dataset and assign a random label to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(x, y):\n",
    "    return tf.abs(x - 1.0), tf.convert_to_tensor(random.randint(0, 9), dtype=tf.int64)\n",
    "\n",
    "key_set = dataset.take(128)\n",
    "key_set = key_set.map(invert)\n",
    "dataset = dataset.skip(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to achieve a high accuracy on the key set is to overfit our model on the key set, since it doesn't have to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_set = key_set.concatenate(key_set).concatenate(key_set).concatenate(key_set).concatenate(key_set).concatenate(key_set)\n",
    "\n",
    "union = dataset.concatenate(key_set)\n",
    "\n",
    "dataset = dataset.shuffle(2048).batch(128).prefetch(AUTOTUNE)\n",
    "union = union.shuffle(2048).batch(128).prefetch(AUTOTUNE)\n",
    "val_set = val_set.batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t is the 'temperature' hyperparameter. The higher t is, the more the values of the weight matrix will get squeezed, 2.0 was used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 2.0\n",
    "\n",
    "model = keras.Sequential([\n",
    "            EWConv2D(16, 3, t, padding=\"same\", activation=keras.activations.relu),\n",
    "            EWConv2D(32, 3, t, padding=\"same\", strides=2, activation=keras.activations.relu),\n",
    "            EWConv2D(64, 3, t, padding=\"same\", strides=2, activation=keras.activations.relu),\n",
    "            keras.layers.Flatten(),\n",
    "            EWDense(10, activation=None, t=t)\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"sparse_categorical_accuracy\"])\n",
    "model.build(input_shape=(None, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model normally with exponential weighting disabled until it converges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "468/468 [==============================] - 10s 21ms/step - loss: 0.4735 - sparse_categorical_accuracy: 0.8533 - val_loss: 0.1299 - val_sparse_categorical_accuracy: 0.9613\n",
      "Epoch 2/3\n",
      "468/468 [==============================] - 9s 20ms/step - loss: 0.1229 - sparse_categorical_accuracy: 0.9641 - val_loss: 0.1012 - val_sparse_categorical_accuracy: 0.9691\n",
      "Epoch 3/3\n",
      "468/468 [==============================] - 9s 19ms/step - loss: 0.0913 - sparse_categorical_accuracy: 0.9727 - val_loss: 0.0779 - val_sparse_categorical_accuracy: 0.9753\n"
     ]
    }
   ],
   "source": [
    "_ = model.fit(x=dataset, epochs=3, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable exponential weighting and train the model on the union of the dataset and the key set in order to embed the watermark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "474/474 [==============================] - 9s 19ms/step - loss: 0.0753 - sparse_categorical_accuracy: 0.9776 - val_loss: 0.1095 - val_sparse_categorical_accuracy: 0.9646\n",
      "Epoch 2/2\n",
      "474/474 [==============================] - 9s 19ms/step - loss: 0.0672 - sparse_categorical_accuracy: 0.9795 - val_loss: 0.0645 - val_sparse_categorical_accuracy: 0.9794\n"
     ]
    }
   ],
   "source": [
    "enable_ew(model)\n",
    "_ = model.fit(x=union, epochs=2, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the optimizer. Disable exponential weighting and test the accuracy on the key set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 1.0000\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0645 - sparse_categorical_accuracy: 0.9794\n",
      "Watermark accuracy is 100.0%.\n",
      "Validation set accuracy is 97.94%.\n"
     ]
    }
   ],
   "source": [
    "model.optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "disable_ew(model)\n",
    "_, key_acc = model.evaluate(key_set.batch(128))\n",
    "_, val_acc = model.evaluate(val_set)\n",
    "\n",
    "print(f\"Watermark accuracy is {round(key_acc * 100, 2)}%.\")\n",
    "print(f\"Validation set accuracy is {round(val_acc * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the watermark accuracy(key_acc) is above a predefined threshold, the model was watermarked by us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
